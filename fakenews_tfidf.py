# -*- coding: utf-8 -*-
"""fakenews_tfidf.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WGM-or-L5sQ5ajT4rKVJ3dmuijJ0j4Og
"""

import numpy as np
import pandas as pd 
import regex as re

!pip3 install nltk

import nltk

df=pd.read_csv('/content/fakenewsdataset.zip')

df

df = pd.get_dummies(df, columns=['label'], drop_first=True)

df

df.drop('title' , axis=1 , inplace=True)

df

def regex_operations(string):
    string = re.sub(r"[^\w(),|!?\'\`\:\-\.;\$%#]", " ", string)
    string = re.sub(r"\'s", " is", string)
    string = re.sub(r"\'ve", " have", string)
    string = re.sub(r"n\'t", " not", string)
    string = re.sub(r"\'re", " are", string)
    string = re.sub(r"\'d", " would", string)
    string = re.sub(r"\'ll", " will", string)
    string = re.sub(r"(?<=\w)\.\.\.", " ... ", string)
    string = re.sub(r"(?<=\w)\.", " . ", string)
    string = re.sub(r"(?<=\w),", " , ", string)
    string = re.sub(r"(?<=\w);", " ; ", string)
    string = re.sub(r"(?<=\w)!", " ! ", string)
    string = re.sub(r"\((?=\w)", " ( ", string)
    string = re.sub(r"(?<=\w)\)", " ) ", string)
    string = re.sub(r"(?<=\w)\?", " ? ", string)
    string = re.sub(r"\s{2,}", " ", string)
    string = re.sub(r'\W+,. ', ' ', string)
    string =  re.sub(r'<.*?>' ,' ' ,string)
    string = re.sub(r'[^\w\s]', " ",string)
    string = re.sub('[0-9\n]',' ',string)
    return string.strip()

df['text']=df['text'].apply(regex_operations)

df['text'][1]

def convert_to_lower(org):
  cleantext=org.lower()
  return cleantext
df['text']=df['text'].apply(convert_to_lower)

df['text'][1]

from nltk.stem.porter import PorterStemmer
stemming_model=PorterStemmer()

def stem_words(org):
  cleantext=[]
  for word in org:
    cleantext.append(stemming_model.stem(word))
  new=cleantext[:]
  cleantext.clear()
  return new
df['text']=df['text'].apply(stem_words)

def join_back(org):
  return "".join(org)

df['text']=df['text'].apply(join_back)

df.head()

df.to_csv('preprocessed_fakenews.csv')



sentences = df['text'].tolist()

sentences

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf_v=TfidfVectorizer(max_features=5000,ngram_range=(1,3))
x=tfidf_v.fit_transform(sentences).toarray()

x.shape

y=df['label_REAL']

y

tfidf_v.get_feature_names()[:20]

tfidf_v.get_params()

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=2021)

count_df = pd.DataFrame(x_train, columns=tfidf_v.get_feature_names())

count_df.head()

from sklearn.naive_bayes import GaussianNB
gnb_model=GaussianNB()

gnb_model.fit(x_train, y_train)
gnb_pred = gnb_model.predict(x_test)

from sklearn.metrics import classification_report

print(classification_report(y_test,gnb_pred))

from sklearn.tree import DecisionTreeClassifier

dt_model=DecisionTreeClassifier()

dt_model.fit(x_train, y_train)
dt_pred = dt_model.predict(x_test)

print(classification_report(y_test,dt_pred))

